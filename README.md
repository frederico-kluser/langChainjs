# LangChainTS

[![License: ISC](https://img.shields.io/badge/License-ISC-blue.svg)](https://opensource.org/licenses/ISC)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.8.3-blue)](https://www.typescriptlang.org/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3.21-green)](https://www.langchain.com/)

Uma biblioteca TypeScript que oferece uma API unificada e consistente para interagir com diferentes provedores de modelos de linguagem (LLMs) atravÃ©s do LangChain.

## ğŸŒŸ CaracterÃ­sticas

- ğŸ”„ **API Unificada**: Interface simples e consistente para todos os modelos de linguagem
- ğŸ§© **MÃºltiplos Provedores**: Suporte a Claude, OpenAI, Gemini, DeepSeek e Ollama
- ğŸ“Š **Respostas Estruturadas**: Obtenha respostas em formato JSON com schemas personalizados
- ğŸŒ **Suporte a Idiomas**: Prompts e respostas em PortuguÃªs e InglÃªs
- ğŸ› ï¸ **Altamente ConfigurÃ¡vel**: Controle tokens, temperatura e outros parÃ¢metros
- ğŸ” **TypeScript Nativo**: Tipos fortes e suporte a inferÃªncia de tipos
- ğŸ§ª **Tratamento de Erros**: Respostas de erro consistentes e informativas

## ğŸ“‹ Requisitos

- Node.js 18 ou superior
- TypeScript 5.x
- Chaves de API configuradas para os serviÃ§os que pretende utilizar

## ğŸš€ InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/frederico-kluser/langchaints.git

# Navegue atÃ© o diretÃ³rio
cd langchaints

# Instale as dependÃªncias
npm install

# Configure as variÃ¡veis de ambiente
cp .env.example .env
# Edite o arquivo .env com suas chaves de API
```

## âš™ï¸ ConfiguraÃ§Ã£o

Crie um arquivo `.env` na raiz do projeto com as chaves de API necessÃ¡rias:

```
# Anthropic API Key (Claude)
ANTHROPIC_API_KEY=your_anthropic_api_key

# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key

# Google API Key (Gemini)
GEMINI_API_KEY=your_gemini_api_key

# DeepSeek API Key
DEEPSEEK_API_KEY=your_deepseek_api_key
```

## ğŸ” Uso BÃ¡sico

```typescript
import { getAIResponse, ModelType } from 'langchaints';

// Resposta simples com o modelo padrÃ£o (Claude)
const resposta = await getAIResponse("Qual a capital do Brasil?");
console.log(resposta); // "BrasÃ­lia Ã© a capital do Brasil..."

// Usando OpenAI
const openaiResponse = await getAIResponse(
  "Quem foi Albert Einstein?", 
  ModelType.OPENAI
);

// Com temperatura personalizada
const creativeResponse = await getAIResponse(
  "Crie uma histÃ³ria curta sobre um gato aventureiro",
  ModelType.CLAUDE,
  { temperature: 0.9 }
);
```

## ğŸŒ Suporte a MÃºltiplos Idiomas

```typescript
// Exemplo em portuguÃªs (padrÃ£o)
const resposta = await getAIResponse(
  "Qual a capital do Brasil?",
  ModelType.CLAUDE
);

// Exemplo em inglÃªs
const response = await getAIResponse(
  "What is the capital of Brazil?",
  ModelType.CLAUDE,
  { language: 'en' }
);
```

## ğŸ“Š Respostas Estruturadas

```typescript
// Defina um schema personalizado
const customSchema = {
  nome: "nome completo da pessoa",
  idade: "idade em anos",
  profissao: "ocupaÃ§Ã£o principal"
};

// T serÃ¡ inferido como o tipo do objeto personalizado
const resposta = await getAIResponse<{ 
  nome: string, 
  idade: number, 
  profissao: string 
}>(
  "Dados de JoÃ£o Silva, 32 anos, engenheiro",
  ModelType.CLAUDE,
  { outputSchema: customSchema }
);

// Acesso direto Ã s propriedades
console.log(resposta.nome); // "JoÃ£o Silva"
console.log(resposta.idade); // 32
console.log(resposta.profissao); // "engenheiro"

// Com schema personalizado em inglÃªs
const englishSchema = {
  capital: "name of the capital city",
  country: "name of the country",
  population: "approximate population of the capital"
};

const result = await getAIResponse<{
  capital: string, 
  country: string,
  population: number
}>(
  "What is the capital of Brazil?",
  ModelType.CLAUDE,
  { 
    language: 'en',
    outputSchema: englishSchema
  }
);

console.log(result.capital); // "BrasÃ­lia"
console.log(result.population); // 3094325
```

## ğŸ§ª Exemplos de Uso

### Executando a aplicaÃ§Ã£o principal
```bash
npm run start
```

### Executando testes
```bash
# Testar todos os modelos
npm run test:all

# Testar todos os modelos com resposta em formato JSON
npm run test:json
```

## ğŸ—ï¸ Arquitetura

O projeto segue uma arquitetura modular com os seguintes componentes principais:

- **LLM Service**: ServiÃ§o central que gerencia todos os provedores
- **LLM Providers**: ImplementaÃ§Ãµes especÃ­ficas para cada modelo (Claude, OpenAI, etc.)
- **Utils**: FunÃ§Ãµes auxiliares para formataÃ§Ã£o, parsing e extraÃ§Ã£o de JSON
- **Types**: DefiniÃ§Ãµes de tipos e interfaces comuns

```
src/
â”œâ”€â”€ llm/                  # ImplementaÃ§Ãµes de provedores
â”‚   â”œâ”€â”€ claude-provider.ts
â”‚   â”œâ”€â”€ openai-provider.ts
â”‚   â”œâ”€â”€ gemini-provider.ts
â”‚   â”œâ”€â”€ deepseek-provider.ts
â”‚   â”œâ”€â”€ ollama-provider.ts
â”‚   â””â”€â”€ factory.ts        # Factory para criar provedores
â”œâ”€â”€ service/
â”‚   â””â”€â”€ llm-service.ts    # ServiÃ§o central
â”œâ”€â”€ types.ts              # DefiniÃ§Ãµes de tipos
â”œâ”€â”€ utils.ts              # FunÃ§Ãµes auxiliares
â””â”€â”€ index.ts              # Ponto de entrada principal
testes/
â”œâ”€â”€ resultados/           # DiretÃ³rio com resultados dos testes
â”œâ”€â”€ runAll.ts             # Script para testar todos os modelos
â””â”€â”€ runAllJSON.ts         # Script para testar respostas JSON
```

## ğŸ“„ ConfiguraÃ§Ã£o do Modelo

```typescript
export interface ModelConfig {
  maxTokens?: number;     // Limite de tokens na resposta
  temperature?: number;   // Controle de criatividade (0.0-1.0)
  outputSchema?: Record<string, any>; // Schema para respostas estruturadas
  language?: string;      // Idioma para prompts e respostas ('pt' ou 'en')
}

// Tipo de resposta
export type LLMResponse<T = string> = T;
```

## ğŸ”„ Modelos Suportados

A biblioteca suporta os seguintes modelos:

- **Claude (Anthropic)**: Modelos Claude atuais
- **OpenAI**: GPT-3.5 Turbo e GPT-4
- **Gemini (Google)**: Gemini 1.5 Flash
- **DeepSeek**: Modelo DeepSeek Chat
- **Ollama**: Modelos locais (llama3.1)

## ğŸ§© ExtensÃ£o

Para adicionar um novo provedor:

1. Crie uma nova classe de provedor em `src/llm/[nome]-provider.ts`
2. Implemente a interface `ILLMProvider`
3. Adicione o provedor ao factory em `src/llm/factory.ts`
4. Adicione o provedor aos scripts de teste em `testes/runAll.ts` e `testes/runAllJSON.ts`

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, sinta-se Ã  vontade para enviar pull requests ou abrir issues para discutir melhorias, correÃ§Ãµes de bugs ou novas funcionalidades.

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a [ISC License](LICENSE).

## ğŸ‘¨â€ğŸ’» Autor

**Frederico Guilherme Kluser de Oliveira**

---

Desenvolvido com â¤ï¸ usando TypeScript e LangChain.